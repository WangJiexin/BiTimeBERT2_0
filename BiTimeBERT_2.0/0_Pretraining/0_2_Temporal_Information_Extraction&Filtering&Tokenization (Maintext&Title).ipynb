{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da12f2e3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Temporal_Information_Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b06667",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Temporal_Information_Extraction (maintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c183307",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "import pyarrow.feather as feather\n",
    "from pytorch_pretrained_bert import BertTokenizer, BasicTokenizer\n",
    "from babel.dates import format_date, format_datetime, format_time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import multiprocessing\n",
    "import tqdm.notebook as tqdm\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "multi_process_num = multiprocessing.cpu_count()\n",
    "from sutime import SUTime\n",
    "sutime = SUTime(mark_time_ranges=True, include_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa43645",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ranked_data_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/*.feather\"):\n",
    "    ranked_data_files_paths.append(file_name)\n",
    "ranked_data_files_paths = sorted(ranked_data_files_paths)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26977c64",
   "metadata": {
    "code_folding": [
     11
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sutime_results(pd_input):\n",
    "    row_idx, row = pd_input\n",
    "    main_text = row[\"maintext\"]\n",
    "    date_publish = row[\"date_publish\"]\n",
    "    date_publish=date_publish[:4]+'-'+date_publish[4:6]+'-'+date_publish[6:8]\n",
    "    docid = row[\"ID\"]\n",
    "    sutime_result=sutime.parse(main_text,reference_date=date_publish)\n",
    "    if row_idx%1000==0:\n",
    "        print(row_idx,end=\"; \")\n",
    "    return (docid, sutime_result)\n",
    "\n",
    "for file_idx, data_file_path in enumerate(ranked_data_files_paths):\n",
    "    start_t = time.time()\n",
    "    filtered_pd = feather.read_feather(data_file_path)\n",
    "    print(data_file_path,len(filtered_pd))\n",
    "    filtered_pd = filtered_pd.rename(columns={'file_id': 'ID', 'pub': 'date_publish', 'body_text': 'maintext'})\n",
    "    docid2tempinfor_dict=dict()\n",
    "    save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/0_Maintext_TempInfor_Files/{file_idx}_maintext_sutime_tempinfor.pickle\"\n",
    "    result_dict = dict(thread_map(sutime_results, filtered_pd.iterrows(), tqdm_class=tqdm.tqdm, max_workers=multi_process_num))\n",
    "    pickle.dump(result_dict, open(save_file, \"wb\"))\n",
    "    end_t = time.time()\n",
    "    print(f\"Time use: {end_t-start_t}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f0d0f85",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for file_idx, data_file_path in enumerate(ranked_data_files_paths):\n",
    "    print(data_file_path)\n",
    "    start_t = time.time()\n",
    "    filtered_pd = feather.read_feather(data_file_path).iloc[:100]\n",
    "    filtered_pd = filtered_pd.rename(columns={'file_id': 'ID', 'pub': 'date_publish', 'body_text': 'maintext'})\n",
    "    docid2tempinfor_dict=dict()\n",
    "    save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/0_Maintext_TempInfor_Files/{file_idx}_sutime_tempinfor.pickle\"\n",
    "    for row_idx, row in filtered_pd.iterrows():\n",
    "        main_text = row[\"maintext\"]\n",
    "        date_publish = row[\"date_publish\"]\n",
    "        date_publish=date_publish[:4]+'-'+date_publish[4:6]+'-'+date_publish[6:8]\n",
    "        docid = row[\"ID\"]\n",
    "        sutime_result=sutime.parse(main_text,reference_date=date_publish)\n",
    "        docid2tempinfor_dict[docid]=sutime_result\n",
    "    pickle.dump(docid2tempinfor_dict, open(save_file, \"wb\"))\n",
    "    end_t = time.time()\n",
    "    print(f\"Time use: {end_t-start_t}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee619249",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Temporal_Information_Extraction (title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51e458",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "import pyarrow.feather as feather\n",
    "from pytorch_pretrained_bert import BertTokenizer, BasicTokenizer\n",
    "from babel.dates import format_date, format_datetime, format_time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import multiprocessing\n",
    "import tqdm.notebook as tqdm\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "multi_process_num = multiprocessing.cpu_count()\n",
    "from sutime import SUTime\n",
    "sutime = SUTime(mark_time_ranges=True, include_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f00f0f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ranked_titleinfor_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/3_EntInfor_AND_SentIdxInfor-Titles_Files/*.feather\"):\n",
    "    ranked_titleinfor_files_paths.append(file_name)\n",
    "ranked_titleinfor_files_paths = sorted(ranked_titleinfor_files_paths)\n",
    "docid2timestamp_dict = pickle.load(open(\"./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/0_docid2timestamp.pickle\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18b511",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sutime_results(pd_input):\n",
    "    row_idx, row = pd_input\n",
    "    main_text = row[\"maintext\"]\n",
    "    date_publish = row[\"date_publish\"]\n",
    "    date_publish=date_publish[:4]+'-'+date_publish[4:6]+'-'+date_publish[6:8]\n",
    "    docid = row[\"ID\"]\n",
    "    sutime_result=sutime.parse(main_text,reference_date=date_publish)\n",
    "    if row_idx%1000==0:\n",
    "        print(row_idx,end=\"; \")\n",
    "    return (docid, sutime_result)\n",
    "\n",
    "for file_idx, data_file_path in enumerate(ranked_titleinfor_files_paths):\n",
    "    start_t = time.time()\n",
    "    filtered_pd = feather.read_feather(data_file_path).iloc[:10]\n",
    "    print(data_file_path,len(filtered_pd))\n",
    "    filtered_pd = filtered_pd.rename(columns={'file_id': 'ID', 'pub': 'date_publish', 'title_text': 'maintext'})\n",
    "    filtered_pd['date_publish']= filtered_pd['ID'].map(docid2timestamp_dict)\n",
    "    save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/1_Titletext_TempInfor_Files/{file_idx}_titletext_sutime_tempinfor.pickle\"\n",
    "    result_dict = dict(thread_map(sutime_results, filtered_pd.iterrows(), tqdm_class=tqdm.tqdm, max_workers=multi_process_num))\n",
    "    pickle.dump(result_dict, open(save_file, \"wb\"))\n",
    "    end_t = time.time()\n",
    "    print(f\"Time use: {end_t-start_t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3309e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fefdff1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Temporal_Information_Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93662e24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238229b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "maintext_tempinfor_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/0_Maintext_TempInfor_Files/*.pickle\"):\n",
    "    maintext_tempinfor_files_paths.append(file_name)\n",
    "maintext_tempinfor_files_paths = sorted(maintext_tempinfor_files_paths)\n",
    "\n",
    "titletext_tempinfor_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/1_Titletext_TempInfor_Files/*.pickle\"):\n",
    "    titletext_tempinfor_files_paths.append(file_name)\n",
    "titletext_tempinfor_files_paths = sorted(titletext_tempinfor_files_paths)\n",
    "\n",
    "ranked_data_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/*.feather\"):\n",
    "    ranked_data_files_paths.append(file_name)\n",
    "ranked_data_files_paths = sorted(ranked_data_files_paths)\n",
    "\n",
    "ranked_entAndsent_files_paths = []\n",
    "for file_name in glob.glob(\"0_Corpus/0_NYT_Data_Extraction/2_EntInfor_AND_SentIdxInfor_Files/*.feather\"):\n",
    "    ranked_entAndsent_files_paths.append(file_name)\n",
    "ranked_entAndsent_files_paths = sorted(ranked_entAndsent_files_paths)    \n",
    "\n",
    "ranked_titleinfor_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/3_EntInfor_AND_SentIdxInfor-Titles_Files/*.feather\"):\n",
    "    ranked_titleinfor_files_paths.append(file_name)\n",
    "ranked_titleinfor_files_paths = sorted(ranked_titleinfor_files_paths)   \n",
    "\n",
    "print(len(maintext_tempinfor_files_paths), len(titletext_tempinfor_files_paths), len(ranked_data_files_paths), len(ranked_entAndsent_files_paths), len(ranked_titleinfor_files_paths))\n",
    "\n",
    "for file_idx, fpaths in enumerate(zip(maintext_tempinfor_files_paths, ranked_data_files_paths, titletext_tempinfor_files_paths,ranked_entAndsent_files_paths, ranked_titleinfor_files_paths)):\n",
    "    fpath1,fpath2,fpath3,fpath4,fpath5= fpaths\n",
    "    f_idx1 = fpath1.split(\"/\")[-1].split(\"_\")[0]\n",
    "    f_idx2 = fpath2.split(\"/\")[-1].split(\"_\")[0]\n",
    "    f_idx3 = fpath3.split(\"/\")[-1].split(\"_\")[0]\n",
    "    f_idx4 = fpath4.split(\"/\")[-1].split(\"_\")[0]\n",
    "    f_idx5 = fpath5.split(\"/\")[-1].split(\"_\")[0]\n",
    "    if f_idx1==f_idx2==f_idx3==f_idx4==f_idx5==str(file_idx):\n",
    "        continue\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9695bd6a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def skip_temp_fun(text):\n",
    "    skip_temp_tag = False\n",
    "    if re.search('(.*weeks?)|(.*weekends?)|(.*years?)|(.*decades?)', text.lower()):\n",
    "        skip_temp_tag = True\n",
    "    if re.search('(several)|(a few)', text.lower()):\n",
    "        skip_temp_tag = True\n",
    "    if re.search(\"(the day)|(the other day)|(millennium)|(century)|(centuries)\",text.lower()):\n",
    "        skip_temp_tag = True\n",
    "    if re.search(\"(more)|(less)\",text.lower()):\n",
    "        skip_temp_tag = True\n",
    "    return skip_temp_tag\n",
    "\n",
    "def sutimetemp_filtering_fun(tempinfor):\n",
    "    temp_list = []\n",
    "    for temp_dict in tempinfor:\n",
    "        try:\n",
    "            if temp_dict['type']==\"DATE\":\n",
    "                temp_text = temp_dict['text']\n",
    "                skip_temp_tag = skip_temp_fun(temp_text)\n",
    "                if skip_temp_tag:\n",
    "                    continue\n",
    "                t_infor = [temp_text, temp_dict['value'], temp_dict['start'], temp_dict['end']]\n",
    "                if re.match(r'(([1-2]\\d{3})-((0[1-9])|(1[0-2]))-((0[1-9])|([1,2][0-9])|(3[0-1])))',temp_dict['value']):\n",
    "                    t_infor.append(\"day\")\n",
    "                    temp_list.append(t_infor)\n",
    "                    continue\n",
    "                if re.match(r'(([1-2]\\d{3})-((0[1-9])|(1[0-2])))',temp_dict['value']):\n",
    "                    t_infor.append(\"month\")\n",
    "                    temp_list.append(t_infor)\n",
    "                    continue\n",
    "                if re.match(r'([1-2]\\d{3})',temp_dict['value']):\n",
    "                    t_infor.append(\"year\")\n",
    "                    temp_list.append(t_infor)\n",
    "                    continue\n",
    "        except:\n",
    "            continue\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd8486c3",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def return_filtered_temp_infor(text,raw_sutimetemp_infor, sent_infor_list, ent_infor_list):\n",
    "    sent_pos_list = []\n",
    "    sentidx2newsentidx = dict()\n",
    "    newsentidx = -1\n",
    "    for sent_i,sent_infor in enumerate(sent_infor_list):  \n",
    "        b_pos, e_pos = list(map(int, sent_infor.split(\"_X_\")))\n",
    "        sent_pos_list.append([b_pos, e_pos])\n",
    "        line_text = text[b_pos:e_pos].strip()\n",
    "        if len(line_text)!=0:\n",
    "            newsentidx+=1\n",
    "        sentidx2newsentidx[sent_i] = newsentidx\n",
    "    \n",
    "    filtered_sutimetemp_infor = []\n",
    "    if len(raw_sutimetemp_infor)!=0:\n",
    "        sutimetemp_infor = sutimetemp_filtering_fun(raw_sutimetemp_infor)\n",
    "        enttemp_pos  = []\n",
    "        for entinfor in ent_infor_list:\n",
    "            if entinfor.endswith('DATE'):\n",
    "                enttemp_pos.append(list(map(int,entinfor.split(\"_X_\")[1:3])))\n",
    "        for temp_infor in sutimetemp_infor:\n",
    "            ent_pos_tag = False\n",
    "            temp_text = temp_infor[0]\n",
    "            b_pos = temp_infor[2]\n",
    "            e_pos = temp_infor[3]\n",
    "            temp_grad = temp_infor[4]\n",
    "            for ent_pos in enttemp_pos:\n",
    "                if b_pos==ent_pos[0] and e_pos==ent_pos[1]:\n",
    "                    ent_pos_tag = True\n",
    "                    break\n",
    "            if ent_pos_tag:\n",
    "                for sent_i,sent_pos in enumerate(sent_pos_list):\n",
    "                    if sent_pos[0]<=b_pos<=e_pos<=sent_pos[1]:\n",
    "                        t_infor = [temp_text, temp_grad, [b_pos, e_pos], [sent_i, sentidx2newsentidx[sent_i]]]\n",
    "                        filtered_sutimetemp_infor.append(t_infor)\n",
    "                        break\n",
    "    return filtered_sutimetemp_infor, [sent_pos_list,sentidx2newsentidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a552c8f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/0_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; ./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/1_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; ./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/2_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; ./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/3_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; ./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/4_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; ./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/5_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; ./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/6_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; ./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/7_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; ./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/8_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; ./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/9_NYT_filtered_data.feather\n",
      "0; 10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; "
     ]
    }
   ],
   "source": [
    "docid2tempinfor_dict = dict()\n",
    "for file_idx, fpaths in enumerate(zip(maintext_tempinfor_files_paths,titletext_tempinfor_files_paths, ranked_data_files_paths, ranked_entAndsent_files_paths, ranked_titleinfor_files_paths)):\n",
    "    maintext_tempinfor_file, titletext_tempinfor_file, data_file, entAndsent_file, titleinfor_file = fpaths\n",
    "    docid2maintext_tempinfor_dict = pickle.load(open(maintext_tempinfor_file,'rb'))\n",
    "    docid2titletext_tempinfor_dict = pickle.load(open(titletext_tempinfor_file,'rb'))\n",
    "    \n",
    "    text_data = feather.read_feather(data_file)\n",
    "    entAndsent_data = feather.read_feather(entAndsent_file)\n",
    "    titleinfor_data = feather.read_feather(titleinfor_file)\n",
    "    print(data_file)\n",
    "    \n",
    "    text_data = text_data.rename(columns={'body_text': 'main_text', 'pub': 'date_publish', 'file_id': 'ID'})\n",
    "    entAndsent_data = entAndsent_data.rename(columns={'ent_infor': 'main_ent_infor', 'sent_infor': 'main_sent_infor'})\n",
    "    titleinfor_data = titleinfor_data.rename(columns={'ent_infor': 'title_ent_infor', 'sent_infor': 'title_sent_infor'})\n",
    "    merged_pd = pd.merge(text_data, entAndsent_data, on='ID')\n",
    "    merged_pd = pd.merge(merged_pd, titleinfor_data, on='ID')\n",
    "    merged_pd = merged_pd[[\"main_text\", \"main_ent_infor\", \"main_sent_infor\", \"title_text\", \"title_ent_infor\", \"title_sent_infor\", \"date_publish\", \"ID\"]]\n",
    "    for row_idx, row in merged_pd.iterrows():\n",
    "        if row_idx%10000==0:\n",
    "            print(row_idx,end=\"; \")\n",
    "        main_text = row[\"main_text\"]\n",
    "        text_sent_infor = row[\"main_sent_infor\"]\n",
    "        text_ent_infor = row[\"main_ent_infor\"]\n",
    "        title_text = row[\"title_text\"]\n",
    "        title_ent_infor = row[\"title_ent_infor\"]\n",
    "        title_sent_infor = row[\"title_sent_infor\"]\n",
    "        date_publish = row[\"date_publish\"]\n",
    "        docid = row[\"ID\"]\n",
    "        main_sutimetemp_infor = docid2maintext_tempinfor_dict[docid]\n",
    "        main_filtered_temp_infor, main_sent_pos_list = return_filtered_temp_infor(main_text, main_sutimetemp_infor, text_sent_infor, text_ent_infor)\n",
    "        title_sutimetemp_infor = docid2titletext_tempinfor_dict[docid]\n",
    "        title_filtered_temp_infor, title_sent_pos_list = return_filtered_temp_infor(title_text, title_sutimetemp_infor, title_sent_infor, title_ent_infor)\n",
    "        docid2tempinfor_dict[docid] = [main_filtered_temp_infor, main_sent_pos_list, title_filtered_temp_infor, title_sent_pos_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1661442",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/2_Extracted_TempInfor/filtered_tempinfor_sutime.pickle\"\n",
    "pickle.dump(docid2tempinfor_dict, open(save_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e1a6c",
   "metadata": {},
   "source": [
    "# Temporal_Information_Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9155ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/2_Extracted_TempInfor/filtered_tempinfor_sutime.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m docid2tempinfor_dict_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/2_Extracted_TempInfor/filtered_tempinfor_sutime.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m docid2tempinfor_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocid2tempinfor_dict_file\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docid2tempinfor_dict))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/2_Extracted_TempInfor/filtered_tempinfor_sutime.pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "docid2tempinfor_dict_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/2_Extracted_TempInfor/filtered_tempinfor_sutime.pickle\"\n",
    "docid2tempinfor_dict = pickle.load(open(docid2tempinfor_dict_file,'rb'))\n",
    "print(len(docid2tempinfor_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3eec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1109, 4917, 1222, 1828, 119, 2159, 117, 1150, 1108, 1255, 1107, 6010, 1105, 2120, 1107, 1913, 4319, 117, 1113, 3261, 2054, 117, 1138, 1723, 1117, 1578, 1112, 170, 1558, 2482, 1107, 12488, 1390, 1105, 6581, 1219, 1103, 3281, 119, 2577, 1386, 1119, 1125, 3494, 3523, 2330, 1104, 1103, 1912, 1104, 4289, 1105, 4771, 1785, 2628, 1114, 1103, 2357, 2491, 1105, 1390, 1104, 1199, 1168, 14024, 12488, 2719, 117, 1259, 1103, 1753, 19402, 139, 119, 146, 119, 144, 119, 1105, 17037, 4163, 1665, 156, 21893, 2149, 117, 1150, 1127, 1841, 1107, 2797, 118, 1118, 4598, 1116, 119]\n"
     ]
    }
   ],
   "source": [
    "temp_text = \"\"\" The charges against Mr. Smith, who was born in Brooklyn and raised in Uniondale, on Long Island, have followed his career as a major figure in rap music and videos during the 1990s. Before 2006 he had largely stayed clear of the kind of violence and criminality associated with the personal lives and music of some other pioneering rap artists, including the Notorious B.I.G. and Tupac Shakur, who were killed in drive-by shootings.\"\"\"\n",
    "\n",
    "print(tokenizer.encode(temp_text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a607e445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The charges against Mr . Smith , who was born in Brooklyn and raised in Union ##dale , on Long Island , have followed his career as a major figure in rap music and videos during the 1990s . Before 2006 he had largely stayed clear of the kind of violence and criminal ##ity associated with the personal lives and music of some other pioneering rap artists , including the Not ##orious B . I . G . and Tu ##pa ##c S ##hak ##ur , who were killed in drive - by shooting ##s .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tokenizer.convert_ids_to_tokens(tokenizer.encode(temp_text, add_special_tokens=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0de4767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_batch_prepare_for_model',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_create_trie',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenize',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'basic_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'deprecation_warnings',\n",
       " 'do_basic_tokenize',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'ids_to_tokens',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_for_tokenization',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'tokens_trie',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unique_no_split_tokens',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size',\n",
       " 'wordpiece_tokenizer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c2c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd01531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000; 20000; 30000; 40000; 50000; 60000; 70000; 80000; 90000; 100000; 110000; 120000; 130000; 140000; 150000; 160000; 170000; 180000; 190000; 200000; 210000; 220000; 230000; 240000; 250000; 260000; 270000; 280000; 290000; 300000; 310000; 320000; 330000; 340000; 350000; 360000; 370000; 380000; 390000; 400000; 410000; 420000; 430000; 440000; 450000; 460000; 470000; 480000; 490000; 500000; 510000; 520000; 530000; 540000; 550000; 560000; 570000; 580000; 590000; 600000; 610000; 620000; 630000; 640000; 650000; 660000; 670000; 680000; 690000; 700000; 710000; 720000; 730000; 740000; 750000; 760000; 770000; 780000; 790000; 800000; 810000; 820000; 830000; 840000; 850000; 860000; 870000; 880000; 890000; 900000; 910000; 920000; 930000; 940000; 950000; 960000; 970000; 980000; 990000; 1000000; 1010000; 1020000; 1030000; 1040000; 1050000; 1060000; 1070000; 1080000; 1090000; 1100000; 1110000; 1120000; 1130000; 1140000; 1150000; 1160000; 1170000; 1180000; 1190000; 1200000; 1210000; 1220000; 1230000; 1240000; 1250000; 1260000; 1270000; 1280000; 1290000; 1300000; 1310000; 1320000; 1330000; 1340000; 1350000; 1360000; 1370000; 1380000; 1390000; 1400000; 1410000; 1420000; 1430000; 1440000; 1450000; 1460000; 1470000; 1480000; 1490000; 1500000; 1510000; 1520000; 1530000; 1540000; 1550000; 1560000; 1570000; 1580000; 1590000; 1600000; 1610000; 1620000; 1630000; 1640000; 1650000; "
     ]
    }
   ],
   "source": [
    "docid2temptokenizeinfor_dict = dict()\n",
    "row_idx = 0\n",
    "for docid, tempinfor in docid2tempinfor_dict.items():\n",
    "    row_idx+=1\n",
    "    if row_idx%10000==0:\n",
    "        print(row_idx,end=\"; \")  \n",
    "    main_temp_infor, main_sent_pos_infor, title_temp_infor, title_sent_pos_infor = tempinfor\n",
    "    _,title_sentidx2newsentidx = title_sent_pos_infor\n",
    "    title_sent_num = title_sentidx2newsentidx[len(title_sentidx2newsentidx)-1]+1\n",
    "    tokenize_infor_results = []\n",
    "    for temp_infor in title_temp_infor:\n",
    "        temp_text = temp_infor[0]\n",
    "        temp_grad = temp_infor[1]\n",
    "        temp_sent_idx = temp_infor[3][1]\n",
    "        token_result = tokenizer.encode(temp_text, add_special_tokens=False)\n",
    "        if temp_grad==\"month\" or temp_grad==\"year\":\n",
    "            if len(token_result)>8:\n",
    "                continue\n",
    "        tokenize_infor_results.append([temp_text, temp_grad, token_result, temp_sent_idx])\n",
    "    for temp_infor in main_temp_infor:\n",
    "        temp_text = temp_infor[0]\n",
    "        temp_grad = temp_infor[1]\n",
    "        temp_sent_idx = temp_infor[3][1]+title_sent_num\n",
    "        token_result = tokenizer.encode(temp_text, add_special_tokens=False)\n",
    "        if temp_grad==\"month\" or temp_grad==\"year\":\n",
    "            if len(token_result)>8:\n",
    "                continue\n",
    "        tokenize_infor_results.append([temp_text, temp_grad, token_result, temp_sent_idx])\n",
    "    docid2temptokenizeinfor_dict[docid] = tokenize_infor_results\n",
    "    \n",
    "save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/3_Tempinfor_Tokenize/docid2temptokenizeinfor_dict.pickle\"\n",
    "pickle.dump(docid2temptokenizeinfor_dict, open(save_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d65b0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docid2sentidx2temptokenizeinfor_dict = dict()\n",
    "for docid, temptokenizeinfor in docid2temptokenizeinfor_dict.items():\n",
    "    docid2sentidx2temptokenizeinfor_dict[docid] = dict()\n",
    "    for tempinfor in temptokenizeinfor:\n",
    "        temp_text, temp_grad, token_result, temp_sent_idx = tempinfor\n",
    "        if temp_sent_idx not in docid2sentidx2temptokenizeinfor_dict[docid]:\n",
    "            docid2sentidx2temptokenizeinfor_dict[docid][temp_sent_idx] = []\n",
    "        docid2sentidx2temptokenizeinfor_dict[docid][temp_sent_idx].append([temp_grad, token_result])\n",
    "save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/3_Tempinfor_Tokenize/docid2sentidx2temptokenizeinfor_dict.pickle\"\n",
    "pickle.dump(docid2sentidx2temptokenizeinfor_dict, open(save_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a499ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_temptokenset_dict = {\"day\":dict(), \"month\":dict(), \"year\":dict()}\n",
    "for docid, temptokenizeinfor in docid2temptokenizeinfor_dict.items():\n",
    "    for tempinfor in temptokenizeinfor:\n",
    "        len_v = len(tempinfor[2])\n",
    "        if len_v not in grad_temptokenset_dict[tempinfor[1]]:\n",
    "            grad_temptokenset_dict[tempinfor[1]][len_v] = set()\n",
    "        grad_temptokenset_dict[tempinfor[1]][len_v].add(tuple(tempinfor[2]))\n",
    "        \n",
    "save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/3_Tempinfor_Tokenize/gradlen_temptokenizationset_dict.pickle\"\n",
    "pickle.dump(grad_temptokenset_dict, open(save_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b913a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for grad,len2temptokenset in grad_temptokenset_dict.items():\n",
    "    sort_len = sorted(list(grad_temptokenset_dict[grad].keys()))\n",
    "    for length in sort_len:\n",
    "        if length==1:\n",
    "            continue\n",
    "        grad_temptokenset_dict[grad][length].update(grad_temptokenset_dict[grad][length-1])\n",
    "save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/3_Tempinfor_Tokenize/new_gradlen_temptokenizationset_dict.pickle\"\n",
    "pickle.dump(grad_temptokenset_dict, open(save_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fe2a56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4877"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grad_temptokenset_dict[\"day\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb979f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a66a5e9f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a3f3d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Temporal_Information_Filtering (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e2079",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929de20",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ranked_data_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/*.feather\"):\n",
    "    ranked_data_files_paths.append(file_name)\n",
    "ranked_data_files_paths = sorted(ranked_data_files_paths)\n",
    "\n",
    "ranked_entAndsent_files_paths = []\n",
    "for file_name in glob.glob(\"0_Corpus/0_NYT_Data_Extraction/2_EntInfor_AND_SentIdxInfor_Files/*.feather\"):\n",
    "    ranked_entAndsent_files_paths.append(file_name)\n",
    "ranked_entAndsent_files_paths = sorted(ranked_entAndsent_files_paths)    \n",
    "\n",
    "ranked_titleinfor_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/3_EntInfor_AND_SentIdxInfor-Titles_Files/*.feather\"):\n",
    "    ranked_titleinfor_files_paths.append(file_name)\n",
    "ranked_titleinfor_files_paths = sorted(ranked_titleinfor_files_paths)  \n",
    "\n",
    "print(len(ranked_data_files_paths), len(ranked_entAndsent_files_paths), len(ranked_titleinfor_files_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34d7e8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def return_filtered_temp_infor(text, sent_infor_list, ent_infor_list):\n",
    "    newsentidx = -1\n",
    "    sentidx2newsentidx = dict()\n",
    "    sent_pos_list = []\n",
    "    for sent_i,sent_infor in enumerate(sent_infor_list):  \n",
    "        b_pos, e_pos = list(map(int, sent_infor.split(\"_X_\")))\n",
    "        sent_pos_list.append([b_pos, e_pos])\n",
    "        line_text = text[b_pos:e_pos].strip()\n",
    "        if len(line_text)!=0:\n",
    "            newsentidx+=1\n",
    "        sentidx2newsentidx[sent_i] = newsentidx\n",
    "        \n",
    "    filtered_sutimetemp_infor = []\n",
    "    for entinfor in ent_infor_list:\n",
    "        if entinfor.endswith('DATE'):\n",
    "            temp_text, b_pos, e_pos, ent_type = entinfor.split(\"_X_\")\n",
    "            b_pos,e_pos = int(b_pos), int(e_pos)\n",
    "            for sent_i,sent_pos in enumerate(sent_pos_list):\n",
    "                if sent_pos[0]<=b_pos<=e_pos<=sent_pos[1]:\n",
    "                    filtered_sutimetemp_infor.append([temp_text, [b_pos, e_pos], [sent_i, sentidx2newsentidx[sent_i]]])\n",
    "                    break\n",
    "    return filtered_sutimetemp_infor, [sent_pos_list,sentidx2newsentidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7263b1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "docid2tempinfor_dict = dict()\n",
    "for file_idx, fpaths in enumerate(zip(ranked_data_files_paths, ranked_entAndsent_files_paths, ranked_titleinfor_files_paths)):\n",
    "    data_file, entAndsent_file, titleinfor_file = fpaths\n",
    "    text_data = feather.read_feather(data_file)\n",
    "    entAndsent_data = feather.read_feather(entAndsent_file)\n",
    "    titleinfor_data = feather.read_feather(titleinfor_file)\n",
    "    print(data_file)\n",
    "    \n",
    "    text_data = text_data.rename(columns={'body_text': 'main_text', 'pub': 'date_publish', 'file_id': 'ID'})\n",
    "    entAndsent_data = entAndsent_data.rename(columns={'ent_infor': 'main_ent_infor', 'sent_infor': 'main_sent_infor'})\n",
    "    titleinfor_data = titleinfor_data.rename(columns={'ent_infor': 'title_ent_infor', 'sent_infor': 'title_sent_infor'})\n",
    "    merged_pd = pd.merge(text_data, entAndsent_data, on='ID')\n",
    "    merged_pd = pd.merge(merged_pd, titleinfor_data, on='ID')\n",
    "    merged_pd = merged_pd[[\"main_text\", \"main_ent_infor\", \"main_sent_infor\", \"title_text\", \"title_ent_infor\", \"title_sent_infor\", \"date_publish\", \"ID\"]]\n",
    "    for row_idx, row in merged_pd.iterrows():\n",
    "        if row_idx%10000==0:\n",
    "            print(row_idx,end=\"; \")\n",
    "        main_text = row[\"main_text\"]\n",
    "        text_sent_infor = row[\"main_sent_infor\"]\n",
    "        text_ent_infor = row[\"main_ent_infor\"]\n",
    "        title_text = row[\"title_text\"]\n",
    "        title_ent_infor = row[\"title_ent_infor\"]\n",
    "        title_sent_infor = row[\"title_sent_infor\"]\n",
    "        date_publish = row[\"date_publish\"]\n",
    "        docid = row[\"ID\"]\n",
    "        main_filtered_temp_infor, main_sent_pos_infor = return_filtered_temp_infor(main_text, text_sent_infor, text_ent_infor)\n",
    "        title_filtered_temp_infor, title_sent_pos_infor = return_filtered_temp_infor(title_text, title_sent_infor, title_ent_infor)\n",
    "        docid2tempinfor_dict[docid] = [main_filtered_temp_infor, main_sent_pos_infor, title_filtered_temp_infor, title_sent_pos_infor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2d315",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/2_Extracted_TempInfor/filtered_tempinfor_ner.pickle\"\n",
    "pickle.dump(docid2tempinfor_dict, open(save_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331706c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Temporal_Information_Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b8283",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "docid2tempinfor_dict_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/2_Extracted_TempInfor/filtered_tempinfor_ner.pickle\"\n",
    "docid2tempinfor_dict = pickle.load(open(docid2tempinfor_dict_file,'rb'))\n",
    "print(len(docid2tempinfor_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132815cd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "docid2temptokenizeinfor_dict = dict()\n",
    "row_idx = 0\n",
    "for docid, tempinfor in docid2tempinfor_dict.items():\n",
    "    row_idx+=1\n",
    "    if row_idx%10000==0:\n",
    "        print(row_idx,end=\"; \")\n",
    "    main_temp_infor, main_sent_pos_infor, title_temp_infor, title_sent_pos_infor = tempinfor\n",
    "    _,title_sentidx2newsentidx = title_sent_pos_infor\n",
    "    title_sent_num = title_sentidx2newsentidx[len(title_sentidx2newsentidx)-1]+1\n",
    "    tokenize_infor_results = []\n",
    "    for temp_infor in title_temp_infor:\n",
    "        temp_text = temp_infor[0]\n",
    "        temp_sent_idx = temp_infor[2][1]\n",
    "        token_result = tokenizer.encode(temp_text, add_special_tokens=False)\n",
    "        tokenize_infor_results.append([token_result, temp_sent_idx])\n",
    "    for temp_infor in main_temp_infor:\n",
    "        temp_text = temp_infor[0]\n",
    "        temp_sent_idx = temp_infor[2][1]+title_sent_num\n",
    "        token_result = tokenizer.encode(temp_text, add_special_tokens=False)\n",
    "        tokenize_infor_results.append([token_result, temp_sent_idx])\n",
    "    docid2temptokenizeinfor_dict[docid] = tokenize_infor_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b7108e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_file = f\"./0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/3_Tempinfor_Tokenize/docid2temptokenizeinfor_ner_dict.pickle\"\n",
    "pickle.dump(docid2temptokenizeinfor_dict, open(save_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0897f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e6b7d5d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd73463",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652347 3\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "docid2sentidx2temptokenizeinfor_dict_file = '0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/3_Tempinfor_Tokenize/docid2sentidx2temptokenizeinfor_dict.pickle'\n",
    "docid2sentidx2temptokenizeinfor_dict = pickle.load(open(docid2sentidx2temptokenizeinfor_dict_file,'rb'))\n",
    "gradlen_temptokenizationset_dict_file = '0_Corpus/1_Pretraining_Preprocessing/3_Temporal_Information_Data/3_Tempinfor_Tokenize/gradlen_temptokenizationset_dict.pickle'\n",
    "gradlen_temptokenizationset_dict = pickle.load(open(gradlen_temptokenizationset_dict_file,'rb'))\n",
    "print(len(docid2sentidx2temptokenizeinfor_dict), len(gradlen_temptokenizationset_dict))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
