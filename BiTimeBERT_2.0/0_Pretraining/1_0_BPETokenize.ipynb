{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a674d132",
   "metadata": {},
   "source": [
    "# BPE Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c58932",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BPE Tokenization (Content, Sent-per-line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d6c899",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import iso8601\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import pyarrow.feather as feather\n",
    "from pytorch_pretrained_bert import BertTokenizer, BasicTokenizer\n",
    "from babel.dates import format_date, format_datetime, format_time\n",
    "multi_process_num = multiprocessing.cpu_count()\n",
    "\n",
    "ranked_data_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/*.feather\"):\n",
    "    ranked_data_files_paths.append(file_name)\n",
    "ranked_data_files_paths = sorted(ranked_data_files_paths)    \n",
    "\n",
    "ranked_entAndsent_files_paths = []\n",
    "for file_name in glob.glob(\"0_Corpus/0_NYT_Data_Extraction/2_EntInfor_AND_SentIdxInfor_Files/*.feather\"):\n",
    "    ranked_entAndsent_files_paths.append(file_name)\n",
    "ranked_entAndsent_files_paths = sorted(ranked_entAndsent_files_paths)    \n",
    "\n",
    "for file_idx, fpaths in enumerate(zip(ranked_data_files_paths,ranked_entAndsent_files_paths)):\n",
    "    fpath1,fpath2 = fpaths\n",
    "    f_idx1 = fpath1.split(\"/\")[-1].split(\"_\")[0]\n",
    "    f_idx2 = fpath2.split(\"/\")[-1].split(\"_\")[0]\n",
    "    if f_idx1==f_idx2==str(file_idx):\n",
    "        continue\n",
    "    else:\n",
    "        raise\n",
    "print(len(ranked_data_files_paths),len(ranked_entAndsent_files_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba560bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filtered_pd = feather.read_feather(ranked_data_files_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a617ff",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize(cased_lines, tokenizer, basic_tokenizer, worker_id, batch_offset):\n",
    "    sents = []\n",
    "    for cased_line in cased_lines:\n",
    "        tokens = basic_tokenizer.tokenize(cased_line)\n",
    "        split_tokens = []\n",
    "        for token in tokens:\n",
    "            subtokens = tokenizer.tokenize(token)\n",
    "            split_tokens += subtokens\n",
    "        if len(split_tokens)==0 and cased_line!=\"\":\n",
    "            continue\n",
    "        sents.append(split_tokens)\n",
    "    return worker_id, sents, batch_offset\n",
    "\n",
    "def get_chunks(fpaths_list, chunk_size):\n",
    "    data_file_path, sentinfor_file_path = fpaths_list\n",
    "    chunk = []\n",
    "    doc_chunk_num = 0\n",
    "    doc_chunk_size = chunk_size\n",
    "    filtered_pd = feather.read_feather(data_file_path)\n",
    "    filtered_pd = filtered_pd.rename(columns={'file_id': 'ID', 'pub': 'date_publish', 'body_text': 'maintext'})\n",
    "    sentinfor_pd = feather.read_feather(sentinfor_file_path)\n",
    "    combined_pd = pd.merge(filtered_pd, sentinfor_pd, on='ID')\n",
    "    assert len(filtered_pd)==len(sentinfor_pd)==len(combined_pd)\n",
    "    for row_idx, row in combined_pd.iterrows():\n",
    "        main_text = row[\"maintext\"]\n",
    "        sent_infor_list = row[\"sent_infor\"]\n",
    "        docid = row[\"ID\"]\n",
    "        sent_text_list = []\n",
    "        for sent_infor in sent_infor_list:\n",
    "            sent_infor_beg, sent_infor_end = list(map(int, sent_infor.split(\"_X_\")))\n",
    "            sent_text = main_text[sent_infor_beg:sent_infor_end]\n",
    "            sent_text_list.append(sent_text)\n",
    "        for line in sent_text_list:\n",
    "            line_text = line.strip()\n",
    "            if len(line_text)!=0:\n",
    "                chunk.append(line_text)\n",
    "        chunk.append(\"\")\n",
    "        doc_chunk_num+=1\n",
    "        if doc_chunk_num==doc_chunk_size:\n",
    "            yield chunk\n",
    "            doc_chunk_num=0\n",
    "            chunk = []\n",
    "    yield chunk\n",
    "    \n",
    "def process(fpaths_list, chunk_method, output_file, bert_model_type='bert-base-cased', total=100000000, chunk_size=10000, workers=12):\n",
    "    results = list(range(workers))\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_type)\n",
    "    basic_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "    fout = open(output_file, 'w')\n",
    "    offset = 0\n",
    "    def merge_fn(result):\n",
    "        worker_id, tokenized, batch_offset = result\n",
    "        results[worker_id] = tokenized, batch_offset\n",
    "    for cased_lines in tqdm(get_chunks(fpaths_list, chunk_size), total=total//chunk_size):\n",
    "        pool = multiprocessing.Pool()\n",
    "        size = (len(cased_lines) // workers) if len(cased_lines) % workers == 0 else ( 1 + (len(cased_lines) // workers))\n",
    "        for i in range(workers):\n",
    "            start = i * size\n",
    "            pool.apply_async(tokenize, args = (cased_lines[start:start+size], tokenizer, basic_tokenizer, i, start), callback = merge_fn)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        for lines, batch_offset in results:\n",
    "            for line in lines:\n",
    "                fout.write(' '.join(line) + '\\n')\n",
    "        offset += len(cased_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8b2ebc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing 0th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:15<80:45:13, 29.12s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 1th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:42<85:18:07, 30.76s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 2th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:47<86:02:18, 31.03s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 3th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:52<86:55:10, 31.34s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 4th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:44<85:33:25, 30.85s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 5th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:26<82:42:06, 29.82s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 6th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:28<82:53:48, 29.89s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 7th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:27<82:45:30, 29.84s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 8th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:36<84:10:21, 30.35s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 9th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:44<85:36:31, 30.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file_idx, fpaths in enumerate(zip(ranked_data_files_paths,ranked_entAndsent_files_paths)):\n",
    "    fdata_file_path,fentAndsent_path = fpaths\n",
    "    assert(fdata_file_path.split(\"/\")[-1].split(\"_\")[0]==fentAndsent_path.split(\"/\")[-1].split(\"_\")[0]==str(file_idx))\n",
    "    save_corpus_file = f\"0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/{file_idx}_corpus.txt\"\n",
    "    if os.path.exists(save_corpus_file):\n",
    "        print(f\"Have finished {file_idx}th processing.\")\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        continue\n",
    "    print(f\"Start processing {file_idx}th data.\")\n",
    "    process(fpaths, get_chunks, save_corpus_file, workers = multi_process_num)\n",
    "    print(\"-----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61833e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BPE Tokenization (Title&Content, Sent-per-line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d1f164",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import iso8601\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import pyarrow.feather as feather\n",
    "from pytorch_pretrained_bert import BertTokenizer, BasicTokenizer\n",
    "from babel.dates import format_date, format_datetime, format_time\n",
    "multi_process_num = multiprocessing.cpu_count()\n",
    "\n",
    "ranked_data_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/*.feather\"):\n",
    "    ranked_data_files_paths.append(file_name)\n",
    "ranked_data_files_paths = sorted(ranked_data_files_paths)    \n",
    "\n",
    "ranked_entAndsent_files_paths = []\n",
    "for file_name in glob.glob(\"0_Corpus/0_NYT_Data_Extraction/2_EntInfor_AND_SentIdxInfor_Files/*.feather\"):\n",
    "    ranked_entAndsent_files_paths.append(file_name)\n",
    "ranked_entAndsent_files_paths = sorted(ranked_entAndsent_files_paths)    \n",
    "\n",
    "ranked_titleinfor_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/3_EntInfor_AND_SentIdxInfor-Titles_Files/*.feather\"):\n",
    "    ranked_titleinfor_files_paths.append(file_name)\n",
    "ranked_titleinfor_files_paths = sorted(ranked_titleinfor_files_paths)    \n",
    "\n",
    "for file_idx, fpaths in enumerate(zip(ranked_data_files_paths,ranked_entAndsent_files_paths,ranked_titleinfor_files_paths)):\n",
    "    fpath1,fpath2,fpath3 = fpaths\n",
    "    f_idx1 = fpath1.split(\"/\")[-1].split(\"_\")[0]\n",
    "    f_idx2 = fpath2.split(\"/\")[-1].split(\"_\")[0]\n",
    "    f_idx3 = fpath3.split(\"/\")[-1].split(\"_\")[0]\n",
    "    if f_idx1==f_idx2==f_idx3==str(file_idx):\n",
    "        continue\n",
    "    else:\n",
    "        raise\n",
    "print(len(ranked_data_files_paths),len(ranked_entAndsent_files_paths),len(ranked_titleinfor_files_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8324d8",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize(cased_lines, tokenizer, basic_tokenizer, worker_id, batch_offset):\n",
    "    sents = []\n",
    "    for cased_line in cased_lines:\n",
    "        tokens = basic_tokenizer.tokenize(cased_line)\n",
    "        split_tokens = []\n",
    "        for token in tokens:\n",
    "            subtokens = tokenizer.tokenize(token)\n",
    "            split_tokens += subtokens\n",
    "        if len(split_tokens)==0 and cased_line!=\"\":\n",
    "            continue\n",
    "        sents.append(split_tokens)\n",
    "    return worker_id, sents, batch_offset\n",
    "\n",
    "def get_chunks(fpaths_list, chunk_size):\n",
    "    data_file_path, sentinfor_file_path,titleinfor_file_path = fpaths_list\n",
    "    chunk = []\n",
    "    doc_chunk_num = 0\n",
    "    doc_chunk_size = chunk_size\n",
    "    filtered_pd = feather.read_feather(data_file_path)\n",
    "    filtered_pd = filtered_pd.rename(columns={'file_id': 'ID', 'pub': 'date_publish', 'body_text': 'maintext'})\n",
    "    sentinfor_pd = feather.read_feather(sentinfor_file_path)\n",
    "    title_pd = feather.read_feather(titleinfor_file_path)\n",
    "    title_pd = title_pd.rename(columns={'ent_infor': 'title_ent_infor', 'sent_infor': 'title_sent_infor', 'token_num': 'title_token_num'})\n",
    "    \n",
    "    combined_pd = pd.merge(filtered_pd, sentinfor_pd, on='ID')\n",
    "    combined_pd = pd.merge(combined_pd, title_pd, on='ID')\n",
    "    assert len(filtered_pd)==len(sentinfor_pd)==len(combined_pd)\n",
    "    for row_idx, row in combined_pd.iterrows():\n",
    "        main_text = row[\"maintext\"]\n",
    "        sent_infor_list = row[\"sent_infor\"]\n",
    "        title_sent_infor = row[\"title_sent_infor\"]\n",
    "        title_text = row[\"title_text\"]\n",
    "        docid = row[\"ID\"]\n",
    "        sent_text_list = []\n",
    "        for sent_infor in title_sent_infor:\n",
    "            sent_infor_beg, sent_infor_end = list(map(int, sent_infor.split(\"_X_\")))\n",
    "            sent_text = title_text[sent_infor_beg:sent_infor_end]\n",
    "            sent_text_list.append(sent_text)\n",
    "        for sent_infor in sent_infor_list:\n",
    "            sent_infor_beg, sent_infor_end = list(map(int, sent_infor.split(\"_X_\")))\n",
    "            sent_text = main_text[sent_infor_beg:sent_infor_end]\n",
    "            sent_text_list.append(sent_text)\n",
    "        for line in sent_text_list:\n",
    "            line_text = line.strip()\n",
    "            if len(line_text)!=0:\n",
    "                chunk.append(line_text)\n",
    "        chunk.append(\"\")\n",
    "        doc_chunk_num+=1\n",
    "        if doc_chunk_num==doc_chunk_size:\n",
    "            yield chunk\n",
    "            doc_chunk_num=0\n",
    "            chunk = []\n",
    "    yield chunk\n",
    "    \n",
    "def process(fpaths_list, chunk_method, output_file, bert_model_type='bert-base-cased', total=100000000, chunk_size=10000, workers=12):\n",
    "    results = list(range(workers))\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_type)\n",
    "    basic_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "    fout = open(output_file, 'w')\n",
    "    offset = 0\n",
    "    def merge_fn(result):\n",
    "        worker_id, tokenized, batch_offset = result\n",
    "        results[worker_id] = tokenized, batch_offset\n",
    "    for cased_lines in tqdm(get_chunks(fpaths_list, chunk_size), total=total//chunk_size):\n",
    "        pool = multiprocessing.Pool()\n",
    "        size = (len(cased_lines) // workers) if len(cased_lines) % workers == 0 else ( 1 + (len(cased_lines) // workers))\n",
    "        for i in range(workers):\n",
    "            start = i * size\n",
    "            pool.apply_async(tokenize, args = (cased_lines[start:start+size], tokenizer, basic_tokenizer, i, start), callback = merge_fn)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        for lines, batch_offset in results:\n",
    "            for line in lines:\n",
    "                fout.write(' '.join(line) + '\\n')\n",
    "        offset += len(cased_lines)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f19c39e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing 0th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:45<85:42:24, 30.91s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 1th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:57<87:40:37, 31.62s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 2th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:44<85:30:36, 30.84s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 3th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:44<85:37:03, 30.87s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 4th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:37<84:21:18, 30.42s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 5th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:39<84:42:48, 30.55s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 6th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:41<85:01:15, 30.66s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 7th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:39<84:41:30, 30.54s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 8th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:43<85:23:48, 30.80s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 9th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:52<86:51:02, 31.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file_idx, fpaths in enumerate(zip(ranked_data_files_paths,ranked_entAndsent_files_paths,ranked_titleinfor_files_paths)):\n",
    "    fdata_file_path,fentAndsent_path,ftitleinfor_path = fpaths\n",
    "    assert(fdata_file_path.split(\"/\")[-1].split(\"_\")[0]==fentAndsent_path.split(\"/\")[-1].split(\"_\")[0]==ftitleinfor_path.split(\"/\")[-1].split(\"_\")[0]==str(file_idx))\n",
    "    save_corpus_file = f\"0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/{file_idx}_corpus.txt\"\n",
    "    if os.path.exists(save_corpus_file):\n",
    "        print(f\"Have finished {file_idx}th processing.\")\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        continue\n",
    "    print(f\"Start processing {file_idx}th data.\")\n",
    "    process(fpaths, get_chunks, save_corpus_file, workers = multi_process_num)\n",
    "    print(\"-----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37eba9",
   "metadata": {},
   "source": [
    "## BPE Tokenization (Title&Content&ID, Sent-per-line,ID is appened as last sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774173e0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import iso8601\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import pyarrow.feather as feather\n",
    "from pytorch_pretrained_bert import BertTokenizer, BasicTokenizer\n",
    "from babel.dates import format_date, format_datetime, format_time\n",
    "multi_process_num = multiprocessing.cpu_count()\n",
    "\n",
    "ranked_data_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/*.feather\"):\n",
    "    ranked_data_files_paths.append(file_name)\n",
    "ranked_data_files_paths = sorted(ranked_data_files_paths)    \n",
    "\n",
    "ranked_entAndsent_files_paths = []\n",
    "for file_name in glob.glob(\"0_Corpus/0_NYT_Data_Extraction/2_EntInfor_AND_SentIdxInfor_Files/*.feather\"):\n",
    "    ranked_entAndsent_files_paths.append(file_name)\n",
    "ranked_entAndsent_files_paths = sorted(ranked_entAndsent_files_paths)    \n",
    "\n",
    "ranked_titleinfor_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/3_EntInfor_AND_SentIdxInfor-Titles_Files/*.feather\"):\n",
    "    ranked_titleinfor_files_paths.append(file_name)\n",
    "ranked_titleinfor_files_paths = sorted(ranked_titleinfor_files_paths)    \n",
    "\n",
    "for file_idx, fpaths in enumerate(zip(ranked_data_files_paths,ranked_entAndsent_files_paths,ranked_titleinfor_files_paths)):\n",
    "    fpath1,fpath2,fpath3 = fpaths\n",
    "    f_idx1 = fpath1.split(\"/\")[-1].split(\"_\")[0]\n",
    "    f_idx2 = fpath2.split(\"/\")[-1].split(\"_\")[0]\n",
    "    f_idx3 = fpath3.split(\"/\")[-1].split(\"_\")[0]\n",
    "    if f_idx1==f_idx2==f_idx3==str(file_idx):\n",
    "        continue\n",
    "    else:\n",
    "        raise\n",
    "print(len(ranked_data_files_paths),len(ranked_entAndsent_files_paths),len(ranked_titleinfor_files_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1947a618",
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "def tokenize(cased_lines, tokenizer, basic_tokenizer, worker_id, batch_offset):\n",
    "    sents = []\n",
    "    for cased_line in cased_lines:\n",
    "        tokens = basic_tokenizer.tokenize(cased_line)\n",
    "        split_tokens = []\n",
    "        for token in tokens:\n",
    "            subtokens = tokenizer.tokenize(token)\n",
    "            split_tokens += subtokens\n",
    "        if len(split_tokens)==0 and cased_line!=\"\":\n",
    "            continue\n",
    "        sents.append(split_tokens)\n",
    "    return worker_id, sents, batch_offset\n",
    "\n",
    "def get_chunks(fpaths_list, chunk_size):\n",
    "    data_file_path, sentinfor_file_path,titleinfor_file_path = fpaths_list\n",
    "    chunk = []\n",
    "    doc_chunk_num = 0\n",
    "    doc_chunk_size = chunk_size\n",
    "    filtered_pd = feather.read_feather(data_file_path)\n",
    "    filtered_pd = filtered_pd.rename(columns={'file_id': 'ID', 'pub': 'date_publish', 'body_text': 'maintext'})\n",
    "    sentinfor_pd = feather.read_feather(sentinfor_file_path)\n",
    "    title_pd = feather.read_feather(titleinfor_file_path)\n",
    "    title_pd = title_pd.rename(columns={'ent_infor': 'title_ent_infor', 'sent_infor': 'title_sent_infor', 'token_num': 'title_token_num'})\n",
    "    \n",
    "    combined_pd = pd.merge(filtered_pd, sentinfor_pd, on='ID')\n",
    "    combined_pd = pd.merge(combined_pd, title_pd, on='ID')\n",
    "    assert len(filtered_pd)==len(sentinfor_pd)==len(combined_pd)\n",
    "    for row_idx, row in combined_pd.iterrows():\n",
    "        main_text = row[\"maintext\"]\n",
    "        sent_infor_list = row[\"sent_infor\"]\n",
    "        title_sent_infor = row[\"title_sent_infor\"]\n",
    "        title_text = row[\"title_text\"]\n",
    "        docid = row[\"ID\"]\n",
    "        sent_text_list = []\n",
    "        ###append title as the first paragraph###\n",
    "        for sent_infor in title_sent_infor:\n",
    "            sent_infor_beg, sent_infor_end = list(map(int, sent_infor.split(\"_X_\")))\n",
    "            sent_text = title_text[sent_infor_beg:sent_infor_end]\n",
    "            sent_text_list.append(sent_text)\n",
    "        for sent_infor in sent_infor_list:\n",
    "            sent_infor_beg, sent_infor_end = list(map(int, sent_infor.split(\"_X_\")))\n",
    "            sent_text = main_text[sent_infor_beg:sent_infor_end]\n",
    "            sent_text_list.append(sent_text)\n",
    "        ###append docid as the last sentence###\n",
    "        sent_text_list.append(docid)\n",
    "        for line in sent_text_list:\n",
    "            line_text = line.strip()\n",
    "            if len(line_text)!=0:\n",
    "                chunk.append(line_text)\n",
    "        chunk.append(\"\")\n",
    "        doc_chunk_num+=1\n",
    "        if doc_chunk_num==doc_chunk_size:\n",
    "            yield chunk\n",
    "            doc_chunk_num=0\n",
    "            chunk = []\n",
    "    yield chunk\n",
    "    \n",
    "def process(fpaths_list, chunk_method, output_file, bert_model_type='bert-base-cased', total=100000000, chunk_size=10000, workers=12):\n",
    "    results = list(range(workers))\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_type)\n",
    "    basic_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "    fout = open(output_file, 'w')\n",
    "    offset = 0\n",
    "    def merge_fn(result):\n",
    "        worker_id, tokenized, batch_offset = result\n",
    "        results[worker_id] = tokenized, batch_offset\n",
    "    for cased_lines in tqdm(get_chunks(fpaths_list, chunk_size), total=total//chunk_size):\n",
    "        pool = multiprocessing.Pool()\n",
    "        size = (len(cased_lines) // workers) if len(cased_lines) % workers == 0 else ( 1 + (len(cased_lines) // workers))\n",
    "        for i in range(workers):\n",
    "            start = i * size\n",
    "            pool.apply_async(tokenize, args = (cased_lines[start:start+size], tokenizer, basic_tokenizer, i, start), callback = merge_fn)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        for lines, batch_offset in results:\n",
    "            for line in lines:\n",
    "                fout.write(' '.join(line) + '\\n')\n",
    "        offset += len(cased_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377f13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing 0th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:21<81:46:14, 29.49s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 1th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:49<86:26:49, 31.17s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 2th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [09:06<89:11:09, 32.16s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 3th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:53<86:57:15, 31.36s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 4th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:48<86:11:56, 31.08s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 5th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:47<86:00:22, 31.01s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 6th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:47<86:07:14, 31.06s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 7th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:47<86:03:39, 31.03s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 8th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:52<86:50:04, 31.31s/it]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Start processing 9th data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 17/10000 [08:48<86:12:24, 31.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file_idx, fpaths in enumerate(zip(ranked_data_files_paths,ranked_entAndsent_files_paths,ranked_titleinfor_files_paths)):\n",
    "    fdata_file_path,fentAndsent_path,ftitleinfor_path = fpaths\n",
    "    assert(fdata_file_path.split(\"/\")[-1].split(\"_\")[0]==fentAndsent_path.split(\"/\")[-1].split(\"_\")[0]==ftitleinfor_path.split(\"/\")[-1].split(\"_\")[0]==str(file_idx))\n",
    "    save_corpus_file = f\"0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/{file_idx}_corpus.txt\"\n",
    "    if os.path.exists(save_corpus_file):\n",
    "        print(f\"Have finished {file_idx}th processing.\")\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        continue\n",
    "    print(f\"Start processing {file_idx}th data.\")\n",
    "    process(fpaths, get_chunks, save_corpus_file, workers = multi_process_num)\n",
    "    print(\"-----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8feda",
   "metadata": {},
   "source": [
    "# Tokenize Pretraining Corpus (Train-Valid-Test Split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad005ea",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b813d336",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "ranked_corpus_files_paths = []\n",
    "for file_name in glob.glob(\"0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/*.txt\"):\n",
    "    ranked_corpus_files_paths.append(file_name)\n",
    "ranked_corpus_files_paths = sorted(ranked_corpus_files_paths)\n",
    "print(len(ranked_corpus_files_paths))\n",
    "\n",
    "train_corpus_files_list = ranked_corpus_files_paths[:9]\n",
    "valtest_corpus_files_list = ranked_corpus_files_paths[9:]\n",
    "print(len(train_corpus_files_list), len(valtest_corpus_files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f585ef43",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/0_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/1_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/2_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/3_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/4_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/5_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/6_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/7_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/8_corpus.txt: 165234\n"
     ]
    }
   ],
   "source": [
    "train_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/0_corpus_content-only/0_corpus_train.txt\"\n",
    "fout = open(train_corpus_fout, 'w')\n",
    "for train_file_paths in train_corpus_files_list:\n",
    "    doc_num = 0\n",
    "    print(train_file_paths,end=\": \")\n",
    "    corpus_file = open(train_file_paths)\n",
    "    for line in corpus_file:\n",
    "        if line==\"\\n\":\n",
    "            doc_num+=1\n",
    "        fout.write(line)\n",
    "    print(doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6373090f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/9_corpus.txt\n",
      "total_doc_num: 165241\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/0_corpus_content-only_files/9_corpus.txt\n",
      "val_doc_num: 82620\n",
      "test_doc_num: 82621\n"
     ]
    }
   ],
   "source": [
    "val_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/0_corpus_content-only/0_corpus_val.txt\"\n",
    "test_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/0_corpus_content-only/0_corpus_test.txt\"\n",
    "val_fout = open(val_corpus_fout, 'w')\n",
    "test_fout = open(test_corpus_fout, 'w')\n",
    "\n",
    "total_doc_num = 0\n",
    "for valtest_file_paths in valtest_corpus_files_list:\n",
    "    print(valtest_file_paths)\n",
    "    with open(valtest_file_paths) as corpus_file:    \n",
    "        for line in corpus_file:\n",
    "            if line==\"\\n\":\n",
    "                total_doc_num+=1\n",
    "print(\"total_doc_num:\", total_doc_num)\n",
    "\n",
    "split_doc_num = 0\n",
    "for valtest_file_paths in valtest_corpus_files_list:\n",
    "    print(valtest_file_paths)\n",
    "    val_doc_num,test_doc_num = 0,0\n",
    "    with open(valtest_file_paths) as corpus_file:    \n",
    "        for line in corpus_file:\n",
    "            if line==\"\\n\":\n",
    "                split_doc_num+=1\n",
    "            if split_doc_num<=total_doc_num//2:\n",
    "                val_fout.write(line)\n",
    "                if line==\"\\n\":\n",
    "                    val_doc_num+=1\n",
    "            else:\n",
    "                test_fout.write(line)\n",
    "                if line==\"\\n\":\n",
    "                    test_doc_num+=1\n",
    "    print(\"val_doc_num:\", val_doc_num)\n",
    "    print(\"test_doc_num:\", test_doc_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79da535",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4224e997",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1487106 1487106\n"
     ]
    }
   ],
   "source": [
    "train_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/0_corpus_content-only/0_corpus_train.txt\"\n",
    "doc_num1 = 0\n",
    "corpus_file = open(train_corpus_fout)\n",
    "for line in corpus_file:\n",
    "    if line==\"\\n\":\n",
    "        doc_num1+=1\n",
    "print(doc_num1, 165234*9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8405d58",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82620 82620\n"
     ]
    }
   ],
   "source": [
    "val_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/0_corpus_content-only/0_corpus_val.txt\"\n",
    "doc_num2 = 0\n",
    "corpus_file = open(val_corpus_fout)\n",
    "for line in corpus_file:\n",
    "    if line==\"\\n\":\n",
    "        doc_num2+=1\n",
    "print(doc_num2, 165241//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41e47bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82621 82621\n"
     ]
    }
   ],
   "source": [
    "test_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/0_corpus_content-only/0_corpus_test.txt\"\n",
    "doc_num3 = 0\n",
    "corpus_file = open(test_corpus_fout)\n",
    "for line in corpus_file:\n",
    "    if line==\"\\n\":\n",
    "        doc_num3+=1\n",
    "print(doc_num3, 165241-165241//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101c0012",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652347 True\n"
     ]
    }
   ],
   "source": [
    "print(doc_num1+doc_num2+doc_num3,doc_num1+doc_num2+doc_num3==1652347)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a284b4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Title&Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede32d81",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "ranked_corpus_files_paths = []\n",
    "for file_name in glob.glob(\"0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/*.txt\"):\n",
    "    ranked_corpus_files_paths.append(file_name)\n",
    "ranked_corpus_files_paths = sorted(ranked_corpus_files_paths)\n",
    "print(len(ranked_corpus_files_paths))\n",
    "\n",
    "train_corpus_files_list = ranked_corpus_files_paths[:9]\n",
    "valtest_corpus_files_list = ranked_corpus_files_paths[9:]\n",
    "print(len(train_corpus_files_list), len(valtest_corpus_files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefe76b8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/0_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/1_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/2_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/3_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/4_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/5_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/6_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/7_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/8_corpus.txt: 165234\n"
     ]
    }
   ],
   "source": [
    "train_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/1_corpus_titlecontent/0_corpus_train.txt\"\n",
    "fout = open(train_corpus_fout, 'w')\n",
    "for train_file_paths in train_corpus_files_list:\n",
    "    doc_num = 0\n",
    "    print(train_file_paths,end=\": \")\n",
    "    corpus_file = open(train_file_paths)\n",
    "    for line in corpus_file:\n",
    "        if line==\"\\n\":\n",
    "            doc_num+=1\n",
    "        fout.write(line)\n",
    "    print(doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8df14e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/9_corpus.txt\n",
      "total_doc_num: 165241\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/1_corpus_titlecontent_files/9_corpus.txt\n",
      "val_doc_num: 82620\n",
      "test_doc_num: 82621\n"
     ]
    }
   ],
   "source": [
    "val_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/1_corpus_titlecontent/0_corpus_val.txt\"\n",
    "test_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/1_corpus_titlecontent/0_corpus_test.txt\"\n",
    "val_fout = open(val_corpus_fout, 'w')\n",
    "test_fout = open(test_corpus_fout, 'w')\n",
    "\n",
    "total_doc_num = 0\n",
    "for valtest_file_paths in valtest_corpus_files_list:\n",
    "    print(valtest_file_paths)\n",
    "    with open(valtest_file_paths) as corpus_file:    \n",
    "        for line in corpus_file:\n",
    "            if line==\"\\n\":\n",
    "                total_doc_num+=1\n",
    "print(\"total_doc_num:\", total_doc_num)\n",
    "\n",
    "split_doc_num = 0\n",
    "for valtest_file_paths in valtest_corpus_files_list:\n",
    "    print(valtest_file_paths)\n",
    "    val_doc_num,test_doc_num = 0,0\n",
    "    with open(valtest_file_paths) as corpus_file:    \n",
    "        for line in corpus_file:\n",
    "            if line==\"\\n\":\n",
    "                split_doc_num+=1\n",
    "            if split_doc_num<=total_doc_num//2:\n",
    "                val_fout.write(line)\n",
    "                if line==\"\\n\":\n",
    "                    val_doc_num+=1\n",
    "            else:\n",
    "                test_fout.write(line)\n",
    "                if line==\"\\n\":\n",
    "                    test_doc_num+=1\n",
    "    print(\"val_doc_num:\", val_doc_num)\n",
    "    print(\"test_doc_num:\", test_doc_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a866158",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cf9838",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1487106 1487106\n"
     ]
    }
   ],
   "source": [
    "train_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/1_corpus_titlecontent/0_corpus_train.txt\"\n",
    "doc_num1 = 0\n",
    "corpus_file = open(train_corpus_fout)\n",
    "for line in corpus_file:\n",
    "    if line==\"\\n\":\n",
    "        doc_num1+=1\n",
    "print(doc_num1, 165234*9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f1eb04",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82620 82620\n"
     ]
    }
   ],
   "source": [
    "val_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/1_corpus_titlecontent/0_corpus_val.txt\"\n",
    "doc_num2 = 0\n",
    "corpus_file = open(val_corpus_fout)\n",
    "for line in corpus_file:\n",
    "    if line==\"\\n\":\n",
    "        doc_num2+=1\n",
    "print(doc_num2, 165241//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acfe5bdc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82621 82621\n"
     ]
    }
   ],
   "source": [
    "test_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/1_corpus_titlecontent/0_corpus_test.txt\"\n",
    "doc_num3 = 0\n",
    "corpus_file = open(test_corpus_fout)\n",
    "for line in corpus_file:\n",
    "    if line==\"\\n\":\n",
    "        doc_num3+=1\n",
    "print(doc_num3, 165241-165241//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02168676",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652347 True\n"
     ]
    }
   ],
   "source": [
    "print(doc_num1+doc_num2+doc_num3,doc_num1+doc_num2+doc_num3==1652347)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df66be8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Title&Content&ID, ID is appened as last sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbf9a87",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "ranked_corpus_files_paths = []\n",
    "for file_name in glob.glob(\"0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/*.txt\"):\n",
    "    ranked_corpus_files_paths.append(file_name)\n",
    "ranked_corpus_files_paths = sorted(ranked_corpus_files_paths)\n",
    "print(len(ranked_corpus_files_paths))\n",
    "\n",
    "train_corpus_files_list = ranked_corpus_files_paths[:9]\n",
    "valtest_corpus_files_list = ranked_corpus_files_paths[9:]\n",
    "print(len(train_corpus_files_list), len(valtest_corpus_files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcaa83d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/0_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/1_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/2_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/3_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/4_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/5_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/6_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/7_corpus.txt: 165234\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/8_corpus.txt: 165234\n"
     ]
    }
   ],
   "source": [
    "train_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/2_corpus_titlecontent_withdocid/0_corpus_train.txt\"\n",
    "fout = open(train_corpus_fout, 'w')\n",
    "for train_file_paths in train_corpus_files_list:\n",
    "    doc_num = 0\n",
    "    print(train_file_paths,end=\": \")\n",
    "    corpus_file = open(train_file_paths)\n",
    "    for line in corpus_file:\n",
    "        if line==\"\\n\":\n",
    "            doc_num+=1\n",
    "        fout.write(line)\n",
    "    print(doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012f33ca",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/9_corpus.txt\n",
      "total_doc_num: 165241\n",
      "0_Corpus/1_Pretraining_Preprocessing/0_BPE_Tokenization/2_corpus_titlecontent_withdocid_files/9_corpus.txt\n",
      "val_doc_num: 82620\n",
      "test_doc_num: 82621\n"
     ]
    }
   ],
   "source": [
    "val_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/2_corpus_titlecontent_withdocid/0_corpus_val.txt\"\n",
    "test_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/2_corpus_titlecontent_withdocid/0_corpus_test.txt\"\n",
    "val_fout = open(val_corpus_fout, 'w')\n",
    "test_fout = open(test_corpus_fout, 'w')\n",
    "\n",
    "total_doc_num = 0\n",
    "for valtest_file_paths in valtest_corpus_files_list:\n",
    "    print(valtest_file_paths)\n",
    "    with open(valtest_file_paths) as corpus_file:    \n",
    "        for line in corpus_file:\n",
    "            if line==\"\\n\":\n",
    "                total_doc_num+=1\n",
    "print(\"total_doc_num:\", total_doc_num)\n",
    "\n",
    "split_doc_num = 0\n",
    "for valtest_file_paths in valtest_corpus_files_list:\n",
    "    print(valtest_file_paths)\n",
    "    val_doc_num,test_doc_num = 0,0\n",
    "    with open(valtest_file_paths) as corpus_file:    \n",
    "        for line in corpus_file:\n",
    "            if line==\"\\n\":\n",
    "                split_doc_num+=1\n",
    "            if split_doc_num<=total_doc_num//2:\n",
    "                val_fout.write(line)\n",
    "                if line==\"\\n\":\n",
    "                    val_doc_num+=1\n",
    "            else:\n",
    "                test_fout.write(line)\n",
    "                if line==\"\\n\":\n",
    "                    test_doc_num+=1\n",
    "    print(\"val_doc_num:\", val_doc_num)\n",
    "    print(\"test_doc_num:\", test_doc_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f61b16",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a411bd0",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1487106 1487106\n"
     ]
    }
   ],
   "source": [
    "train_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/2_corpus_titlecontent_withdocid/0_corpus_train.txt\"\n",
    "doc_num1 = 0\n",
    "corpus_file = open(train_corpus_fout)\n",
    "for line in corpus_file:\n",
    "    if line==\"\\n\":\n",
    "        doc_num1+=1\n",
    "print(doc_num1, 165234*9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b5eca7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82620 82620\n"
     ]
    }
   ],
   "source": [
    "val_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/2_corpus_titlecontent_withdocid/0_corpus_val.txt\"\n",
    "doc_num2 = 0\n",
    "corpus_file = open(val_corpus_fout)\n",
    "for line in corpus_file:\n",
    "    if line==\"\\n\":\n",
    "        doc_num2+=1\n",
    "print(doc_num2, 165241//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef6060e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82621 82621\n"
     ]
    }
   ],
   "source": [
    "test_corpus_fout = \"0_Corpus/1_Pretraining_Preprocessing/1_Tokenize_Pretraining_Corpus/2_corpus_titlecontent_withdocid/0_corpus_test.txt\"\n",
    "doc_num3 = 0\n",
    "corpus_file = open(test_corpus_fout)\n",
    "for line in corpus_file:\n",
    "    if line==\"\\n\":\n",
    "        doc_num3+=1\n",
    "print(doc_num3, 165241-165241//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d75250",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652347 True\n"
     ]
    }
   ],
   "source": [
    "print(doc_num1+doc_num2+doc_num3,doc_num1+doc_num2+doc_num3==1652347)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
