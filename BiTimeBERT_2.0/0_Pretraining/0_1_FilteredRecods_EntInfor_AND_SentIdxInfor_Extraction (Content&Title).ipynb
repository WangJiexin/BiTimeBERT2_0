{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47dd582f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## EntInfor Extraction  & SentIdxInfo Extraction (content)\n",
    "\n",
    "**Data Size: 1652347**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d37de3f",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import time\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from multiprocessing import Pool, cpu_count\n",
    "multi_process_num = cpu_count()\n",
    "\n",
    "saved_feather_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/*.feather\"):\n",
    "    saved_feather_files_paths.append(file_name)\n",
    "ranked_files_paths = []\n",
    "for file_path in saved_feather_files_paths:\n",
    "    f_idx = int(file_path.split(\"/\")[-1].split(\"_\")[0])\n",
    "    ranked_files_paths.append([f_idx,file_path])\n",
    "ranked_files_paths.sort(key = lambda x: x[0])\n",
    "print(len(ranked_files_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87311de1",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_num=0\n",
    "for file_idx, file_path in ranked_files_paths:\n",
    "    save_file_name = f\"{file_idx}_final_filtered_entinforANDsentidx.feather\"\n",
    "    save_file_path = f\"0_Corpus/0_NYT_Data_Extraction/2_EntInfor_AND_SentIdxInfor_Files/{save_file_name}\"    \n",
    "    if os.path.exists(save_file_path):\n",
    "        print(f\"Have finished {file_idx}th Processing.\")\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        continue\n",
    "    filtered_pd = feather.read_feather(file_path)\n",
    "    filtered_pd = filtered_pd.rename(columns={'file_id': 'ID', 'pub': 'date_publish', 'body_text': 'maintext'})\n",
    "    total_num+=len(filtered_pd)\n",
    "    text_list = filtered_pd[\"maintext\"].tolist()\n",
    "    ID_list = filtered_pd[\"ID\"].tolist()\n",
    "    print(f\"Process {file_idx}th file. Data size:{len(filtered_pd),len(filtered_pd)//1000}\")\n",
    "    assert len(filtered_pd)==len(text_list)\n",
    "    docs = nlp.pipe(text_list, n_process=multi_process_num, batch_size=500)\n",
    "    detailed_ent_infor_list = []\n",
    "    detailed_sent_infor_list = []\n",
    "    for idx,d in enumerate(docs):\n",
    "        if idx%1000==0:\n",
    "            print(idx//1000,end=\"; \")\n",
    "        #1.ent_infor extraction\n",
    "        maintext = text_list[idx]\n",
    "        ent_infor_list = d.ents\n",
    "        detailed_ent_infor = []\n",
    "        for ent_infor in ent_infor_list:\n",
    "            ent_text = ent_infor.text\n",
    "            ent_begin_pos = ent_infor[0].idx\n",
    "            ent_end_pos = ent_infor[0].idx+len(ent_text)\n",
    "            ent_type = ent_infor.label_\n",
    "            assert maintext[ent_begin_pos:ent_end_pos]==ent_text\n",
    "            ent_infor_l = [ent_text,str(ent_begin_pos),str(ent_end_pos),ent_type]\n",
    "            detailed_ent_infor.append(\"_X_\".join(ent_infor_l))            \n",
    "        detailed_ent_infor_list.append(detailed_ent_infor)\n",
    "        #2.sent_infor extraction\n",
    "        sent_infor_list = d.sents\n",
    "        detailed_sent_infor = []\n",
    "        for sent_infor in sent_infor_list:\n",
    "            sent_text = sent_infor.text\n",
    "            sent_begin_pos = sent_infor[0].idx\n",
    "            sent_end_pos = sent_infor[0].idx+len(sent_text)\n",
    "            assert maintext[sent_begin_pos:sent_end_pos]==sent_text\n",
    "            #sent_infor_l = [str(sent_begin_pos),str(sent_end_pos),sent_text]\n",
    "            sent_infor_l = [str(sent_begin_pos),str(sent_end_pos)]\n",
    "            detailed_sent_infor.append(\"_X_\".join(sent_infor_l))\n",
    "        detailed_sent_infor_list.append(detailed_sent_infor)\n",
    "    \n",
    "    assert len(detailed_ent_infor_list)==len(detailed_sent_infor_list)==len(ID_list)\n",
    "    entinfor_sentinfor_list = []\n",
    "    for entinfor, sentinfor, idinfor in zip(detailed_ent_infor_list,detailed_sent_infor_list,ID_list):\n",
    "        entinfor_sentinfor_list.append([entinfor, sentinfor, idinfor])\n",
    "    entinfor_sentinfor_pd = pd.DataFrame(entinfor_sentinfor_list,columns = [\"ent_infor\",\"sent_infor\",\"ID\"])\n",
    "    feather.write_feather(entinfor_sentinfor_pd, save_file_path)\n",
    "\n",
    "    print(f\"Finish processing {len(entinfor_sentinfor_pd)} data.\")\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "print(\"total_num:\",total_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c9c014",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Analysis\n",
    "**Data Size: 1652347**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9a4935",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "from collections import Counter\n",
    "\n",
    "saved_feather_files_paths = []\n",
    "for file_name in glob.glob(\"0_Corpus/0_NYT_Data_Extraction/2_EntInfor_AND_SentIdxInfor_Files/*.feather\"):\n",
    "    saved_feather_files_paths.append(file_name)\n",
    "print(len(saved_feather_files_paths))\n",
    "\n",
    "ranked_entinfor_files_paths = []\n",
    "for file_path in saved_feather_files_paths:\n",
    "    f_idx = int(file_path.split(\"/\")[-1].split(\"_\")[0])\n",
    "    ranked_entinfor_files_paths.append([f_idx,file_path])\n",
    "ranked_entinfor_files_paths.sort(key = lambda x: x[0])\n",
    "print(len(ranked_entinfor_files_paths))\n",
    "\n",
    "fileid_list = []\n",
    "for file_path in ranked_entinfor_files_paths:\n",
    "    news_pd = feather.read_feather(file_path[1])\n",
    "    fileid_list.extend(news_pd[\"ID\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a056758c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652347\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ent_infor</th>\n",
       "      <th>sent_infor</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165238</th>\n",
       "      <td>[American_X_42_X_50_X_NORP, European_X_315_X_3...</td>\n",
       "      <td>[0_X_272, 273_X_496, 496_X_497, 497_X_675, 676...</td>\n",
       "      <td>1265245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165239</th>\n",
       "      <td>[Joyce Van Patten_X_229_X_245_X_PERSON, New Yo...</td>\n",
       "      <td>[0_X_109, 110_X_201, 202_X_399, 399_X_570, 571...</td>\n",
       "      <td>1614637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165240</th>\n",
       "      <td>[Jed_X_119_X_122_X_PERSON, Rhys Ifans_X_161_X_...</td>\n",
       "      <td>[0_X_286, 287_X_384, 385_X_448, 449_X_581, 582...</td>\n",
       "      <td>1624014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ent_infor  \\\n",
       "165238  [American_X_42_X_50_X_NORP, European_X_315_X_3...   \n",
       "165239  [Joyce Van Patten_X_229_X_245_X_PERSON, New Yo...   \n",
       "165240  [Jed_X_119_X_122_X_PERSON, Rhys Ifans_X_161_X_...   \n",
       "\n",
       "                                               sent_infor       ID  \n",
       "165238  [0_X_272, 273_X_496, 496_X_497, 497_X_675, 676...  1265245  \n",
       "165239  [0_X_109, 110_X_201, 202_X_399, 399_X_570, 571...  1614637  \n",
       "165240  [0_X_286, 287_X_384, 385_X_448, 449_X_581, 582...  1624014  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(set(fileid_list)))\n",
    "news_pd.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96baed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abde8d6d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## EntInfor Extraction  & SentIdxInfo Extraction (title)\n",
    "\n",
    "**Data Size: 1652347**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e22a1d",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import time\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from multiprocessing import Pool, cpu_count\n",
    "multi_process_num = cpu_count()\n",
    "\n",
    "saved_feather_files_paths = []\n",
    "for file_name in glob.glob(f\"./0_Corpus/0_NYT_Data_Extraction/1_NYT_FilteredData_Files/*.feather\"):\n",
    "    saved_feather_files_paths.append(file_name)\n",
    "ranked_files_paths = []\n",
    "for file_path in saved_feather_files_paths:\n",
    "    f_idx = int(file_path.split(\"/\")[-1].split(\"_\")[0])\n",
    "    ranked_files_paths.append([f_idx,file_path])\n",
    "ranked_files_paths.sort(key = lambda x: x[0])\n",
    "print(len(ranked_files_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73095222",
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0th file. Data size:(165234, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165234 data.\n",
      "-----------------------------------------------------------------\n",
      "Process 1th file. Data size:(165234, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165234 data.\n",
      "-----------------------------------------------------------------\n",
      "Process 2th file. Data size:(165234, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165234 data.\n",
      "-----------------------------------------------------------------\n",
      "Process 3th file. Data size:(165234, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165234 data.\n",
      "-----------------------------------------------------------------\n",
      "Process 4th file. Data size:(165234, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165234 data.\n",
      "-----------------------------------------------------------------\n",
      "Process 5th file. Data size:(165234, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165234 data.\n",
      "-----------------------------------------------------------------\n",
      "Process 6th file. Data size:(165234, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165234 data.\n",
      "-----------------------------------------------------------------\n",
      "Process 7th file. Data size:(165234, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165234 data.\n",
      "-----------------------------------------------------------------\n",
      "Process 8th file. Data size:(165234, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165234 data.\n",
      "-----------------------------------------------------------------\n",
      "Process 9th file. Data size:(165241, 165)\n",
      "0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102; 103; 104; 105; 106; 107; 108; 109; 110; 111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121; 122; 123; 124; 125; 126; 127; 128; 129; 130; 131; 132; 133; 134; 135; 136; 137; 138; 139; 140; 141; 142; 143; 144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157; 158; 159; 160; 161; 162; 163; 164; 165; Finish processing 165241 data.\n",
      "-----------------------------------------------------------------\n",
      "total_num: 1652347\n",
      "empty_title_num: 0\n"
     ]
    }
   ],
   "source": [
    "total_num = 0\n",
    "empty_title_num = 0\n",
    "for file_idx, file_path in ranked_files_paths:\n",
    "    save_file_name = f\"{file_idx}_final_filtered_entinforANDsentidx-title.feather\"\n",
    "    save_file_path = f\"0_Corpus/0_NYT_Data_Extraction/3_EntInfor_AND_SentIdxInfor-Titles_Files/{save_file_name}\"    \n",
    "    if os.path.exists(save_file_path):\n",
    "        print(f\"Have finished {file_idx}th Processing.\")\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        continue\n",
    "    filtered_pd = feather.read_feather(file_path)\n",
    "    filtered_pd = filtered_pd.rename(columns={'file_id': 'ID', 'pub': 'date_publish', 'body_text': 'maintext'})\n",
    "    total_num+=len(filtered_pd)\n",
    "    text_list = []\n",
    "    for r in filtered_pd[\"title\"].tolist():\n",
    "        if len(r)==0:\n",
    "            text_list.append(\"\")\n",
    "            empty_title_num+=1\n",
    "            continue\n",
    "        text_list.append(r+\".\\n\")\n",
    "    ID_list = filtered_pd[\"ID\"].tolist()\n",
    "    print(f\"Process {file_idx}th file. Data size:{len(filtered_pd),len(filtered_pd)//1000}\")\n",
    "    assert len(filtered_pd)==len(text_list)\n",
    "    docs = nlp.pipe(text_list, n_process=multi_process_num, batch_size=500)\n",
    "    title_token_num = []\n",
    "    detailed_ent_infor_list = []\n",
    "    detailed_sent_infor_list = []\n",
    "    for idx,d in enumerate(docs):\n",
    "        if idx%1000==0:\n",
    "            print(idx//1000,end=\"; \")\n",
    "        #1.ent_infor extraction\n",
    "        title_token_num.append(len(d))\n",
    "        maintext = text_list[idx]\n",
    "        ent_infor_list = d.ents\n",
    "        detailed_ent_infor = []\n",
    "        for ent_infor in ent_infor_list:\n",
    "            ent_text = ent_infor.text\n",
    "            ent_begin_pos = ent_infor[0].idx\n",
    "            ent_end_pos = ent_infor[0].idx+len(ent_text)\n",
    "            ent_type = ent_infor.label_\n",
    "            assert maintext[ent_begin_pos:ent_end_pos]==ent_text\n",
    "            ent_infor_l = [ent_text,str(ent_begin_pos),str(ent_end_pos),ent_type]\n",
    "            detailed_ent_infor.append(\"_X_\".join(ent_infor_l))            \n",
    "        detailed_ent_infor_list.append(detailed_ent_infor)\n",
    "        #2.sent_infor extraction\n",
    "        sent_infor_list = d.sents\n",
    "        detailed_sent_infor = []\n",
    "        for sent_infor in sent_infor_list:\n",
    "            sent_text = sent_infor.text\n",
    "            sent_begin_pos = sent_infor[0].idx\n",
    "            sent_end_pos = sent_infor[0].idx+len(sent_text)\n",
    "            assert maintext[sent_begin_pos:sent_end_pos]==sent_text\n",
    "            #sent_infor_l = [str(sent_begin_pos),str(sent_end_pos),sent_text]\n",
    "            sent_infor_l = [str(sent_begin_pos),str(sent_end_pos)]\n",
    "            detailed_sent_infor.append(\"_X_\".join(sent_infor_l))\n",
    "        detailed_sent_infor_list.append(detailed_sent_infor)\n",
    "    \n",
    "    assert len(detailed_ent_infor_list)==len(detailed_sent_infor_list)==len(ID_list)\n",
    "    entinfor_sentinfor_list = []\n",
    "    for title_text,entinfor, sentinfor, t_num, idinfor in zip(text_list,detailed_ent_infor_list,detailed_sent_infor_list,title_token_num,ID_list):\n",
    "        entinfor_sentinfor_list.append([title_text,entinfor, sentinfor, t_num, idinfor])\n",
    "    entinfor_sentinfor_pd = pd.DataFrame(entinfor_sentinfor_list,columns = [\"title_text\",\"ent_infor\",\"sent_infor\",\"token_num\",\"ID\"])\n",
    "    feather.write_feather(entinfor_sentinfor_pd, save_file_path)\n",
    "    print(f\"Finish processing {len(entinfor_sentinfor_pd)} data.\")\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "print(\"total_num:\",total_num)\n",
    "print(\"empty_title_num:\",empty_title_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1cc446d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_text</th>\n",
       "      <th>ent_infor</th>\n",
       "      <th>sent_infor</th>\n",
       "      <th>token_num</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fugitive Is Arrested Near Rome.\\n</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0_X_31, 31_X_32]</td>\n",
       "      <td>7</td>\n",
       "      <td>1144727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dukakis's Record: A Success Story.\\n</td>\n",
       "      <td>[Dukakis's Record_X_0_X_16_X_ORG, Success Stor...</td>\n",
       "      <td>[0_X_34, 34_X_35]</td>\n",
       "      <td>9</td>\n",
       "      <td>0137146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Woe to Those Displaced by China Dam Project.\\n</td>\n",
       "      <td>[China_X_26_X_31_X_GPE]</td>\n",
       "      <td>[0_X_44, 44_X_45]</td>\n",
       "      <td>10</td>\n",
       "      <td>0823851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title_text  \\\n",
       "0               Fugitive Is Arrested Near Rome.\\n   \n",
       "1            Dukakis's Record: A Success Story.\\n   \n",
       "2  Woe to Those Displaced by China Dam Project.\\n   \n",
       "\n",
       "                                           ent_infor         sent_infor  \\\n",
       "0                                                 []  [0_X_31, 31_X_32]   \n",
       "1  [Dukakis's Record_X_0_X_16_X_ORG, Success Stor...  [0_X_34, 34_X_35]   \n",
       "2                            [China_X_26_X_31_X_GPE]  [0_X_44, 44_X_45]   \n",
       "\n",
       "   token_num       ID  \n",
       "0          7  1144727  \n",
       "1          9  0137146  \n",
       "2         10  0823851  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entinfor_sentinfor_pd.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918937a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Analysis\n",
    "**Data Size: 1652347**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e97e506",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "1652347 1652347\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "from collections import Counter\n",
    "\n",
    "saved_feather_files_paths1 = []\n",
    "for file_name in glob.glob(\"0_Corpus/0_NYT_Data_Extraction/3_EntInfor_AND_SentIdxInfor-Titles_Files/*.feather\"):\n",
    "    saved_feather_files_paths1.append(file_name)\n",
    "saved_feather_files_paths1 = sorted(saved_feather_files_paths1)\n",
    "saved_feather_files_paths2 = []\n",
    "for file_name in glob.glob(\"0_Corpus/0_NYT_Data_Extraction/2_EntInfor_AND_SentIdxInfor_Files/*.feather\"):\n",
    "    saved_feather_files_paths2.append(file_name)\n",
    "saved_feather_files_paths2 = sorted(saved_feather_files_paths2)\n",
    "print(len(saved_feather_files_paths1), len(saved_feather_files_paths2))\n",
    "\n",
    "fileid_list = []\n",
    "token_num_list = []\n",
    "for path1,path2 in zip(saved_feather_files_paths1,saved_feather_files_paths2):\n",
    "    news_pd1 = feather.read_feather(path1)\n",
    "    news_pd2 = feather.read_feather(path2)\n",
    "    assert len(news_pd1)==len(news_pd2)\n",
    "    lenth1 = len(set(news_pd1[\"ID\"].tolist())-set(news_pd2[\"ID\"].tolist()))\n",
    "    lenth2 = len(set(news_pd2[\"ID\"].tolist())-set(news_pd1[\"ID\"].tolist()))\n",
    "    fileid_list.extend(news_pd1[\"ID\"].tolist())\n",
    "    token_num_list.extend(news_pd1[\"token_num\"].tolist())\n",
    "    assert lenth1==lenth2==0\n",
    "print(len(set(fileid_list)),len(token_num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802d0d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
